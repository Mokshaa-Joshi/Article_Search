import streamlit as st

import requests

from bs4 import BeautifulSoup

from urllib.parse import urljoin, urlparse



def normalize_url(url, base_url):

Â  Â  if not url.startswith("http"):

Â  Â  Â  Â  url = urljoin(base_url, url)

Â  Â Â 

Â  Â  parsed_url = urlparse(url)

Â  Â  normalized_url = parsed_url._replace(query='', fragment='').geturl()

Â  Â Â 

Â  Â  return normalized_url



def fetch_article_links(base_url, keyword):

Â  Â  try:

Â  Â  Â  Â  response = requests.get(base_url)

Â  Â  Â  Â  response.raise_for_status()

Â  Â  Â  Â  soup = BeautifulSoup(response.text, 'html.parser')



Â  Â  Â  Â  links = set()



Â  Â  Â  Â  for a in soup.find_all('a', href=True):

Â  Â  Â  Â  Â  Â  if keyword.lower() in a.get('href', '').lower() or keyword.lower() in a.text.lower():

Â  Â  Â  Â  Â  Â  Â  Â  href = a['href']

Â  Â  Â  Â  Â  Â  Â  Â  normalized_url = normalize_url(href, base_url)

Â  Â  Â  Â  Â  Â  Â  Â  links.add((a.text.strip(), normalized_url))



Â  Â  Â  Â  return list(links)

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"An error occurred while fetching links: {e}")

Â  Â  Â  Â  return []



def extract_article(link):

Â  Â  try:

Â  Â  Â  Â  response = requests.get(link)

Â  Â  Â  Â  response.raise_for_status()

Â  Â  Â  Â  soup = BeautifulSoup(response.text, 'html.parser')



Â  Â  Â  Â  date = soup.find('span', class_='time')

Â  Â  Â  Â  article_date = date.get_text(strip=True) if date else "Date not found"



Â  Â  Â  Â  content = soup.find('div', class_='article-detail')

Â  Â  Â  Â  if content:

Â  Â  Â  Â  Â  Â  article_text = "\n".join(p.get_text() for p in content.find_all('p'))

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  paragraphs = soup.find_all('p')

Â  Â  Â  Â  Â  Â  article_text = "\n".join(p.get_text() for p in paragraphs if p.get_text())



Â  Â  Â  Â  return article_date, article_text if article_text else "No article content found."

Â  Â  except Exception as e:

Â  Â  Â  Â  return f"Error extracting article: {e}", ""



def display_articles(links):

Â  Â  if links:

Â  Â  Â  Â  seen_articles = set()



Â  Â  Â  Â  for headline, link in links:

Â  Â  Â  Â  Â  Â  if link not in seen_articles:

Â  Â  Â  Â  Â  Â  Â  Â  with st.expander(f"**{headline}**"):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"[Read Full Article]({link})", unsafe_allow_html=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  date, content = extract_article(link)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"**Published on:** {date}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if content:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"**Article Content:**\n{content}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Article has no content.")

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  seen_articles.add(link)

Â  Â  else:

Â  Â  Â  Â  st.warning("No articles found.")



def main():

Â  Â  st.set_page_config(page_title="Gujarati News Article Finder", page_icon="ðŸ“°")

Â  Â  st.title("Gujarati News Article Finder")



Â  Â  newspaper = st.selectbox("Choose Newspaper", ["Gujarat Midday", "Divya Bhaskar", "Gujarat Samachar"])

Â  Â  keyword = st.text_input("Enter keyword:")



Â  Â  if st.button("Search Articles"):

Â  Â  Â  Â  base_url = {

Â  Â  Â  Â  Â  Â  "Gujarat Midday": "https://www.gujaratimidday.com/",

Â  Â  Â  Â  Â  Â  "Divya Bhaskar": "https://www.divyabhaskar.com/",

Â  Â  Â  Â  Â  Â  "Gujarat Samachar": "https://www.gujaratsamachar.com/"

Â  Â  Â  Â  }[newspaper]



Â  Â  Â  Â  with st.spinner("Searching..."):

Â  Â  Â  Â  Â  Â  links = fetch_article_links(base_url, keyword)

Â  Â  Â  Â  Â  Â  display_articles(links)



if __name__ == "__main__":

Â  Â  main()
